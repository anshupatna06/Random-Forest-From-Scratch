{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763574db-ca54-47ad-99c2-d7c3d1d0c391",
   "metadata": {},
   "source": [
    "# Random Forest From scratch with full explanation of why we our using each and every line of code ğŸŒ´-ğŸŒ´-ğŸŒ´-ğŸŒ´-ğŸŒ´-o/p ğŸ¤” ğŸ«·ğŸ«·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fabd3dd-9cf5-44d1-b96e-f117cc28442c",
   "metadata": {},
   "source": [
    "# Need of using Random Forest is also explained in the last â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸ ğŸ‘ğŸ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7a0b444-266f-4c98-9d01-eac5cf9bf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcce09f-2b95-42d5-a8f0-77334e95c0ba",
   "metadata": {},
   "source": [
    "# load the dataset and split it into dependent and independent features ğŸ‘ğŸ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feab1014-d8ff-47da-95aa-f0741754dabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "874f3b8a-11ce-47d2-a4c7-51f0fe073385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select useful features\n",
    "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
    "\n",
    "# Features and target\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d607c802-9955-43d8-ae4d-29cfcceb0136",
   "metadata": {},
   "source": [
    "# Handle the missing values But Why??? ğŸ˜  -- To increase oue model accuracy,performance,To reduce bias,To prevent inaccurate accuracy ğŸ˜¦ğŸ¥¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c7f08f6-6ac1-4a7b-bbff-0e8051eeaa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24736\\2506929125.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(df['Age'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22357871-8b1a-4e2e-b30a-772430f423c1",
   "metadata": {},
   "source": [
    "# handle the categorical features But Why???? ğŸ˜  --- because \"Gradient Descent\" works properly and smoothly when the data is scaled ğŸ˜¦ğŸ¥¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ce70f17-7735-4db3-957f-fd5193e19f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical feature\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4efb7-90e2-448b-9432-80cc2f8fc185",
   "metadata": {},
   "source": [
    "# Let's build the decision tree by finding the best feature and threshold at each node through iteration in each feature and each data of that feature using Entropy and Information gain ğŸ‘ğŸ«¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44d38a49-2862-468f-84a1-306984802431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        # Stopping conditions\n",
    "        if depth >= self.max_depth or num_samples < self.min_samples_split or len(set(y)) == 1:\n",
    "            return self._majority_vote(y)\n",
    "\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "\n",
    "        # If no valid split found\n",
    "        if best_feature is None:\n",
    "            return self._majority_vote(y)\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        # ğŸ›‘ Avoid creating empty splits\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return self._majority_vote(y)\n",
    "\n",
    "        # Recursive splitting\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return (best_feature, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feature], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def _information_gain(self, y, feature_values, threshold):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_indices = feature_values <= threshold\n",
    "        right_indices = feature_values > threshold\n",
    "\n",
    "        if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        n_left = len(y[left_indices])\n",
    "        n_right = len(y[right_indices])\n",
    "\n",
    "        left_entropy = self._entropy(y[left_indices])\n",
    "        right_entropy = self._entropy(y[right_indices])\n",
    "\n",
    "        child_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        if len(y) == 0:\n",
    "            return 0  # or raise an error, depending on your use case\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs, self.tree) for inputs in X])\n",
    "\n",
    "    def _predict(self, x, tree):\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "\n",
    "        feature, threshold, left, right = tree\n",
    "        if x[feature] <= threshold:\n",
    "            return self._predict(x, left)\n",
    "        else:\n",
    "            return self._predict(x, right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6d93e-a026-4096-9981-ec4d764922eb",
   "metadata": {},
   "source": [
    "# Now let's build the structure of random forest by initializing the number of trees and maximum depth of each tree  ğŸ‘ ğŸ«¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dd6c3f3-e63a-45f9-aa31-d5430ba4b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2, sample_size=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.sample_size = sample_size\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, self.sample_size or n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Majority vote\n",
    "        return [Counter(col).most_common(1)[0][0] for col in tree_preds.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9faa348-cb5a-4c79-aac4-194ebddc8f7d",
   "metadata": {},
   "source": [
    "# Finally,initialize and trian the model ğŸ‘ğŸ‘ğŸ‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c87cdb2-a5c3-497c-9f7c-94e2c917f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24736\\35467184.py:57: RuntimeWarning: invalid value encountered in less_equal\n",
      "  left_indices = feature_values <= threshold\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24736\\35467184.py:58: RuntimeWarning: invalid value encountered in greater\n",
      "  right_indices = feature_values > threshold\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24736\\35467184.py:26: RuntimeWarning: invalid value encountered in less_equal\n",
      "  left_indices = X[:, best_feature] <= best_threshold\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_24736\\35467184.py:27: RuntimeWarning: invalid value encountered in greater\n",
      "  right_indices = X[:, best_feature] > best_threshold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest from scratch: 0.7932960893854749\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train Random Forest\n",
    "forest = RandomForest(n_trees=10, max_depth=10)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = forest.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy of Random Forest from scratch:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12b689c0-68ef-4b44-864f-48ebbece10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence , the accuracy of our model is nearly ~81%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461122a3-65c0-482b-b34e-00412bcf3ad8",
   "metadata": {},
   "source": [
    "# Why do we use Random Forest Model when We have already Decision Tree like model ğŸ¤”ğŸ¤”ğŸ¤” ğŸ§ ğŸ§ ğŸ§ \n",
    "# It's better than Decision Tree because it doesn't construct only one tree in-depth like decision tree,it construct number of trees and then average the predictions based on \"Majority Voting\" made by each tree and hence prevents----\"OVERFITTING\".ğŸ˜¯ğŸ˜¯ ğŸ«¡\n",
    "# we can control the number of trees and depth of each tree in Random Forest ğŸ˜¯ğŸ˜¯\n",
    "# In this way ,we can move from an overfitting model to a Generalised model having low bias and low varinace ğŸ˜¯ğŸ˜¯\n",
    "# Inshort---- (Low Bias,High Variance) is conevrted into-----------------> (Low Bias,Low Varinace) ğŸ«¡ğŸ«¡ğŸ«¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723fe5d-54d5-4a24-b2ba-2c5a85c0a046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
